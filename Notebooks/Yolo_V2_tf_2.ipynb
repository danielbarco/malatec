{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOLO V2 with TensorFlow 2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TOÂ DO**\n",
    "- oversampling of parasitized labels\n",
    "- recut images so that we can use all labels in the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import re\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import xml.etree.ElementTree as ET\n",
    "import imgaug as ia\n",
    "from imgaug import augmenters as iaa\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import bbox_visualizer as bbv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version : 2.4.1\n",
      "GPU : [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print('Tensorflow version : {}'.format(tf.__version__))\n",
    "print('GPU : {}'.format(tf.config.list_physical_devices('GPU')))\n",
    "from tensorflow import keras\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.layers import Concatenate, concatenate, Dropout, LeakyReLU, Reshape, Activation, Conv2D, Input, MaxPooling2D, BatchNormalization, Flatten, Dense, Lambda\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "LABELS           = ('Uninfected', 'Parasitized', 'White_Blood_Cell')\n",
    "IMAGE_H, IMAGE_W = 256, 256\n",
    "GRID_H,  GRID_W  = 8, 8 # GRID size = IMAGE size / 32\n",
    "BOX              = 5\n",
    "CLASS            = len(LABELS)\n",
    "SCORE_THRESHOLD  = 0.5\n",
    "IOU_THRESHOLD    = 0.45\n",
    "ANCHORS          = [0.57273, 0.677385, 1.87446, 2.06253, 3.33843, 5.47434, 7.88282, 3.52778, 9.77052, 9.16828]\n",
    "\n",
    "TRAIN_BATCH_SIZE = 16\n",
    "VAL_BATCH_SIZE   = 16\n",
    "EPOCHS           = 200\n",
    "\n",
    "LAMBDA_NOOBJECT  = 1\n",
    "LAMBDA_OBJECT    = 5\n",
    "LAMBDA_CLASS     = 1\n",
    "LAMBDA_COORD     = 1\n",
    "\n",
    "max_annot        = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and validation directory\n",
    "\n",
    "train_image_folder = '../data/train/image/'\n",
    "train_annot_folder = '../data/train/annotation/'\n",
    "val_image_folder = '../data/val/image/'\n",
    "val_annot_folder = '../data/val/annotation/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Define YOLO model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Keras layer\n",
    "\n",
    "class SpaceToDepth(keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, block_size, **kwargs):\n",
    "        self.block_size = block_size\n",
    "        super(SpaceToDepth, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = inputs\n",
    "        batch, height, width, depth = K.int_shape(x)\n",
    "        batch = -1\n",
    "        reduced_height = height // self.block_size\n",
    "        reduced_width = width // self.block_size\n",
    "        y = K.reshape(x, (batch, reduced_height, self.block_size,\n",
    "                             reduced_width, self.block_size, depth))\n",
    "        z = K.permute_dimensions(y, (0, 1, 3, 2, 4, 5))\n",
    "        t = K.reshape(z, (batch, reduced_height, reduced_width, depth * self.block_size **2))\n",
    "        return t\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        shape =  (input_shape[0], input_shape[1] // self.block_size, input_shape[2] // self.block_size,\n",
    "                  input_shape[3] * self.block_size **2)\n",
    "        return tf.TensorShape(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[3,3,256,512] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:Add]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-65f748b7bbbb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;31m# Layer 13\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConv2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'same'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'conv_13'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_bias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatchNormalization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'norm_13'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLeakyReLU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_in_functional_construction_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m       return self._functional_construction_call(inputs, args, kwargs,\n\u001b[0;32m--> 952\u001b[0;31m                                                 input_list)\n\u001b[0m\u001b[1;32m    953\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m     \u001b[0;31m# Maintains info about the `Layer.call` stack.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_functional_construction_call\u001b[0;34m(self, inputs, args, kwargs, input_list)\u001b[0m\n\u001b[1;32m   1089\u001b[0m         \u001b[0;31m# Check input assumptions set after layer building, e.g. input shape.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1090\u001b[0m         outputs = self._keras_tensor_symbolic_call(\n\u001b[0;32m-> 1091\u001b[0;31m             inputs, input_masks, args, kwargs)\n\u001b[0m\u001b[1;32m   1092\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1093\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_keras_tensor_symbolic_call\u001b[0;34m(self, inputs, input_masks, args, kwargs)\u001b[0m\n\u001b[1;32m    820\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeras_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKerasTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_signature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 822\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_infer_output_signature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    823\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_infer_output_signature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_infer_output_signature\u001b[0;34m(self, inputs, args, kwargs, input_masks)\u001b[0m\n\u001b[1;32m    860\u001b[0m           \u001b[0;31m# overridden).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m           \u001b[0;31m# TODO(kaftan): do we maybe_build here, or have we already done it?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 862\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    863\u001b[0m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    864\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_maybe_build\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2708\u001b[0m         \u001b[0;31m# operations.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2709\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaybe_init_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2710\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint:disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2711\u001b[0m       \u001b[0;31m# We must set also ensure that the layer is marked as built, and the build\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2712\u001b[0m       \u001b[0;31m# shape is stored since user defined build functions may not be calling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/convolutional.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel_constraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m         dtype=self.dtype)\n\u001b[0m\u001b[1;32m    206\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_bias\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m       self.bias = self.add_weight(\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36madd_weight\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, trainable, constraint, use_resource, synchronization, aggregation, **kwargs)\u001b[0m\n\u001b[1;32m    637\u001b[0m         \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m         \u001b[0maggregation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 639\u001b[0;31m         caching_device=caching_device)\n\u001b[0m\u001b[1;32m    640\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mregularizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m       \u001b[0;31m# TODO(fchollet): in the future, this should be handled at the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_add_variable_with_custom_getter\u001b[0;34m(self, name, shape, dtype, initializer, getter, overwrite, **kwargs_for_getter)\u001b[0m\n\u001b[1;32m    808\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m         \u001b[0minitializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 810\u001b[0;31m         **kwargs_for_getter)\n\u001b[0m\u001b[1;32m    811\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    812\u001b[0m     \u001b[0;31m# If we set an initializer and the variable processed it, tracking will not\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer_utils.py\u001b[0m in \u001b[0;36mmake_variable\u001b[0;34m(name, shape, dtype, initializer, trainable, caching_device, validate_shape, constraint, use_resource, collections, synchronization, aggregation, partitioner)\u001b[0m\n\u001b[1;32m    140\u001b[0m       \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m       \u001b[0maggregation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m       shape=variable_shape if variable_shape else None)\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    258\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mVariableV1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_v1_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_v2_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py\u001b[0m in \u001b[0;36m_variable_v1_call\u001b[0;34m(cls, initial_value, trainable, collections, validate_shape, caching_device, name, variable_def, dtype, expected_shape, import_scope, constraint, use_resource, synchronization, aggregation, shape)\u001b[0m\n\u001b[1;32m    219\u001b[0m         \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0maggregation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m         shape=shape)\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m   def _variable_v2_call(cls,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m    197\u001b[0m                         shape=None):\n\u001b[1;32m    198\u001b[0m     \u001b[0;34m\"\"\"Call on Variable class. Useful to force the signature.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m     \u001b[0mprevious_getter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdefault_variable_creator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgetter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creator_stack\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m       \u001b[0mprevious_getter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_make_getter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprevious_getter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mdefault_variable_creator\u001b[0;34m(next_creator, **kwargs)\u001b[0m\n\u001b[1;32m   2616\u001b[0m         \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2617\u001b[0m         \u001b[0maggregation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2618\u001b[0;31m         shape=shape)\n\u001b[0m\u001b[1;32m   2619\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2620\u001b[0m     return variables.RefVariable(\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    262\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_v2_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVariableMetaclass\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, initial_value, trainable, collections, validate_shape, caching_device, name, dtype, variable_def, import_scope, constraint, distribute_strategy, synchronization, aggregation, shape)\u001b[0m\n\u001b[1;32m   1583\u001b[0m           \u001b[0maggregation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1584\u001b[0m           \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1585\u001b[0;31m           distribute_strategy=distribute_strategy)\n\u001b[0m\u001b[1;32m   1586\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1587\u001b[0m   def _init_from_args(self,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36m_init_from_args\u001b[0;34m(self, initial_value, trainable, collections, caching_device, name, dtype, constraint, synchronization, aggregation, distribute_strategy, shape)\u001b[0m\n\u001b[1;32m   1710\u001b[0m           \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Initializer\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_context_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1711\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minit_from_fn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1712\u001b[0;31m               \u001b[0minitial_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitial_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1713\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrackable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCheckpointInitialValue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1714\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_initialize_trackable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/initializers/initializers_v2.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, shape, dtype, **kwargs)\u001b[0m\n\u001b[1;32m    408\u001b[0m     \"\"\"\n\u001b[1;32m    409\u001b[0m     return super(VarianceScaling, self).__call__(\n\u001b[0;32m--> 410\u001b[0;31m         shape, dtype=_get_dtype(dtype), **kwargs)\n\u001b[0m\u001b[1;32m    411\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops_v2.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, shape, dtype, **kwargs)\u001b[0m\n\u001b[1;32m    598\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m       \u001b[0mlimit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3.0\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_random_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_uniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    601\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops_v2.py\u001b[0m in \u001b[0;36mrandom_uniform\u001b[0;34m(self, shape, minval, maxval, dtype)\u001b[0m\n\u001b[1;32m   1080\u001b[0m       \u001b[0mop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_uniform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m     return op(\n\u001b[0;32m-> 1082\u001b[0;31m         shape=shape, minval=minval, maxval=maxval, dtype=dtype, seed=self.seed)\n\u001b[0m\u001b[1;32m   1083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mtruncated_normal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstddev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/random_ops.py\u001b[0m in \u001b[0;36mrandom_uniform\u001b[0;34m(shape, minval, maxval, dtype, seed, name)\u001b[0m\n\u001b[1;32m    306\u001b[0m           \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmaxval\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mminval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    309\u001b[0m     \u001b[0;31m# TODO(b/132092188): C++ shape inference inside functional ops does not\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m     \u001b[0;31m# cross FuncGraph boundaries since that information is only available in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m    325\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6860\u001b[0m   \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6861\u001b[0m   \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6862\u001b[0;31m   \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6863\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6864\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[3,3,256,512] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:Add]"
     ]
    }
   ],
   "source": [
    "# Yolo model (thanks to https://github.com/experiencor/keras-yolo2)\n",
    "\n",
    "input_image = tf.keras.layers.Input((IMAGE_H, IMAGE_W, 3), dtype='float32')\n",
    "\n",
    "# Layer 1\n",
    "x = Conv2D(32, (3,3), strides=(1,1), padding='same', name='conv_1', use_bias=False)(input_image)\n",
    "x = BatchNormalization(name='norm_1')(x)\n",
    "x = LeakyReLU(alpha=0.1)(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "# Layer 2\n",
    "x = Conv2D(64, (3,3), strides=(1,1), padding='same', name='conv_2', use_bias=False)(x)\n",
    "x = BatchNormalization(name='norm_2')(x)\n",
    "x = LeakyReLU(alpha=0.1)(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "# Layer 3\n",
    "x = Conv2D(128, (3,3), strides=(1,1), padding='same', name='conv_3', use_bias=False)(x)\n",
    "x = BatchNormalization(name='norm_3')(x)\n",
    "x = LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "# Layer 4\n",
    "x = Conv2D(64, (1,1), strides=(1,1), padding='same', name='conv_4', use_bias=False)(x)\n",
    "x = BatchNormalization(name='norm_4')(x)\n",
    "x = LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "# Layer 5\n",
    "x = Conv2D(128, (3,3), strides=(1,1), padding='same', name='conv_5', use_bias=False)(x)\n",
    "x = BatchNormalization(name='norm_5')(x)\n",
    "x = LeakyReLU(alpha=0.1)(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "# Layer 6\n",
    "x = Conv2D(256, (3,3), strides=(1,1), padding='same', name='conv_6', use_bias=False)(x)\n",
    "x = BatchNormalization(name='norm_6')(x)\n",
    "x = LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "# Layer 7\n",
    "x = Conv2D(128, (1,1), strides=(1,1), padding='same', name='conv_7', use_bias=False)(x)\n",
    "x = BatchNormalization(name='norm_7')(x)\n",
    "x = LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "# Layer 8\n",
    "x = Conv2D(256, (3,3), strides=(1,1), padding='same', name='conv_8', use_bias=False)(x)\n",
    "x = BatchNormalization(name='norm_8')(x)\n",
    "x = LeakyReLU(alpha=0.1)(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "# Layer 9\n",
    "x = Conv2D(512, (3,3), strides=(1,1), padding='same', name='conv_9', use_bias=False)(x)\n",
    "x = BatchNormalization(name='norm_9')(x)\n",
    "x = LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "# Layer 10\n",
    "x = Conv2D(256, (1,1), strides=(1,1), padding='same', name='conv_10', use_bias=False)(x)\n",
    "x = BatchNormalization(name='norm_10')(x)\n",
    "x = LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "# Layer 11\n",
    "x = Conv2D(512, (3,3), strides=(1,1), padding='same', name='conv_11', use_bias=False)(x)\n",
    "x = BatchNormalization(name='norm_11')(x)\n",
    "x = LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "# Layer 12\n",
    "x = Conv2D(256, (1,1), strides=(1,1), padding='same', name='conv_12', use_bias=False)(x)\n",
    "x = BatchNormalization(name='norm_12')(x)\n",
    "x = LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "# Layer 13\n",
    "x = Conv2D(512, (3,3), strides=(1,1), padding='same', name='conv_13', use_bias=False)(x)\n",
    "x = BatchNormalization(name='norm_13')(x)\n",
    "x = LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "skip_connection = x\n",
    "\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "# Layer 14\n",
    "x = Conv2D(1024, (3,3), strides=(1,1), padding='same', name='conv_14', use_bias=False)(x)\n",
    "x = BatchNormalization(name='norm_14')(x)\n",
    "x = LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "# Layer 15\n",
    "x = Conv2D(512, (1,1), strides=(1,1), padding='same', name='conv_15', use_bias=False)(x)\n",
    "x = BatchNormalization(name='norm_15')(x)\n",
    "x = LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "# Layer 16\n",
    "x = Conv2D(1024, (3,3), strides=(1,1), padding='same', name='conv_16', use_bias=False)(x)\n",
    "x = BatchNormalization(name='norm_16')(x)\n",
    "x = LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "# Layer 17\n",
    "x = Conv2D(512, (1,1), strides=(1,1), padding='same', name='conv_17', use_bias=False)(x)\n",
    "x = BatchNormalization(name='norm_17')(x)\n",
    "x = LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "# Layer 18\n",
    "x = Conv2D(1024, (3,3), strides=(1,1), padding='same', name='conv_18', use_bias=False)(x)\n",
    "x = BatchNormalization(name='norm_18')(x)\n",
    "x = LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "# Layer 19\n",
    "x = Conv2D(1024, (3,3), strides=(1,1), padding='same', name='conv_19', use_bias=False)(x)\n",
    "x = BatchNormalization(name='norm_19')(x)\n",
    "x = LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "# Layer 20\n",
    "x = Conv2D(1024, (3,3), strides=(1,1), padding='same', name='conv_20', use_bias=False)(x)\n",
    "x = BatchNormalization(name='norm_20')(x)\n",
    "x = LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "# Layer 21\n",
    "skip_connection = Conv2D(64, (1,1), strides=(1,1), padding='same', name='conv_21', use_bias=False)(skip_connection)\n",
    "skip_connection = BatchNormalization(name='norm_21')(skip_connection)\n",
    "skip_connection = LeakyReLU(alpha=0.1)(skip_connection)\n",
    "\n",
    "skip_connection = SpaceToDepth(block_size=2)(skip_connection)\n",
    "\n",
    "x = concatenate([skip_connection, x])\n",
    "\n",
    "# Layer 22\n",
    "x = Conv2D(1024, (3,3), strides=(1,1), padding='same', name='conv_22', use_bias=False)(x)\n",
    "x = BatchNormalization(name='norm_22')(x)\n",
    "x = LeakyReLU(alpha=0.1)(x)\n",
    "x = Dropout(0.3)(x) # add dropout\n",
    "\n",
    "# Layer 23\n",
    "x = Conv2D(BOX * (4 + 1 + CLASS), (1,1), strides=(1,1), padding='same', name='conv_23')(x)\n",
    "output = Reshape((GRID_W, GRID_H, BOX, 4 + 1 + CLASS))(x)\n",
    "\n",
    "model = keras.models.Model(input_image, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Load YOLO pretrained weigts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightReader:\n",
    "    def __init__(self, weight_file):\n",
    "        self.offset = 4\n",
    "        self.all_weights = np.fromfile(weight_file, dtype='float32')\n",
    "        \n",
    "    def read_bytes(self, size):\n",
    "        self.offset = self.offset + size\n",
    "        return self.all_weights[self.offset-size:self.offset]\n",
    "    \n",
    "    def reset(self):\n",
    "        self.offset = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_reader = WeightReader('../weights/yolov2.weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_reader.reset()\n",
    "nb_conv = 23\n",
    "\n",
    "for i in range(1, nb_conv+1):\n",
    "    conv_layer = model.get_layer('conv_' + str(i))\n",
    "    conv_layer.trainable = True\n",
    "    \n",
    "    if i < nb_conv:\n",
    "        norm_layer = model.get_layer('norm_' + str(i))\n",
    "        norm_layer.trainable = True\n",
    "        \n",
    "        size = np.prod(norm_layer.get_weights()[0].shape)\n",
    "\n",
    "        beta  = weight_reader.read_bytes(size)\n",
    "        gamma = weight_reader.read_bytes(size)\n",
    "        mean  = weight_reader.read_bytes(size)\n",
    "        var   = weight_reader.read_bytes(size)\n",
    "\n",
    "        weights = norm_layer.set_weights([gamma, beta, mean, var])       \n",
    "        \n",
    "    if len(conv_layer.get_weights()) > 1:\n",
    "        bias   = weight_reader.read_bytes(np.prod(conv_layer.get_weights()[1].shape))\n",
    "        kernel = weight_reader.read_bytes(np.prod(conv_layer.get_weights()[0].shape))\n",
    "        kernel = kernel.reshape(list(reversed(conv_layer.get_weights()[0].shape)))\n",
    "        kernel = kernel.transpose([2,3,1,0])\n",
    "        conv_layer.set_weights([kernel, bias])\n",
    "    else:\n",
    "        kernel = weight_reader.read_bytes(np.prod(conv_layer.get_weights()[0].shape))\n",
    "        kernel = kernel.reshape(list(reversed(conv_layer.get_weights()[0].shape)))\n",
    "        kernel = kernel.transpose([2,3,1,0])\n",
    "        conv_layer.set_weights([kernel])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer   = model.layers[-2] # last convolutional layer\n",
    "layer.trainable = True\n",
    "\n",
    "\n",
    "weights = layer.get_weights()\n",
    "\n",
    "new_kernel = np.random.normal(size=weights[0].shape)/(GRID_H*GRID_W)\n",
    "new_bias   = np.random.normal(size=weights[1].shape)/(GRID_H*GRID_W)\n",
    "\n",
    "layer.set_weights([new_kernel, new_bias])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_annotation(ann_dir, img_dir, labels):\n",
    "    '''\n",
    "    Parse XML files in PASCAL VOC format.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    - ann_dir : annotations files directory\n",
    "    - img_dir : images files directory\n",
    "    - labels : labels list\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    - imgs_name : numpy array of images files path (shape : images count, 1)\n",
    "    - true_boxes : numpy array of annotations for each image (shape : image count, max annotation count, 5)\n",
    "        annotation format : xmin, ymin, xmax, ymax, class\n",
    "        xmin, ymin, xmax, ymax : image unit (pixel)\n",
    "        class = label index\n",
    "    '''\n",
    "    \n",
    "    max_annot = 0\n",
    "    imgs_name = []\n",
    "    annots = []\n",
    "    \n",
    "    # Parse file\n",
    "    for ann in sorted(os.listdir(ann_dir)):\n",
    "        annot_count = 0\n",
    "        boxes = []\n",
    "        tree = ET.parse(ann_dir + ann)\n",
    "        for elem in tree.iter(): \n",
    "            if 'filename' in elem.tag:\n",
    "                imgs_name.append(img_dir + elem.text)\n",
    "            if 'width' in elem.tag:\n",
    "                w = int(elem.text)\n",
    "            if 'height' in elem.tag:\n",
    "                h = int(elem.text)\n",
    "            if 'object' in elem.tag or 'part' in elem.tag:                  \n",
    "                box = np.zeros((5))\n",
    "                for attr in list(elem):\n",
    "                    if 'name' in attr.tag:\n",
    "                        box[4] = labels.index(attr.text) + 1 # 0:label for no bounding box\n",
    "                    if 'bndbox' in attr.tag:\n",
    "                        annot_count += 1\n",
    "                        for dim in list(attr):\n",
    "                            if 'xmin' in dim.tag:\n",
    "                                box[0] = int(round(float(dim.text)))\n",
    "                            if 'ymin' in dim.tag:\n",
    "                                box[1] = int(round(float(dim.text)))\n",
    "                            if 'xmax' in dim.tag:\n",
    "                                box[2] = int(round(float(dim.text)))\n",
    "                            if 'ymax' in dim.tag:\n",
    "                                box[3] = int(round(float(dim.text)))\n",
    "                boxes.append(np.asarray(box))\n",
    "        \n",
    "        if w != IMAGE_W or h != IMAGE_H :\n",
    "            print('Image size error')\n",
    "            break\n",
    "            \n",
    "        annots.append(np.asarray(boxes))\n",
    "        \n",
    "\n",
    "        if annot_count > max_annot:\n",
    "            max_annot = annot_count\n",
    "           \n",
    "    # Rectify annotations boxes : len -> max_annot\n",
    "    imgs_name = np.array(imgs_name)  \n",
    "    true_boxes = np.zeros((imgs_name.shape[0], max_annot, 5))\n",
    "    for idx, boxes in enumerate(annots):\n",
    "        true_boxes[idx, :boxes.shape[0], :5] = boxes\n",
    "        \n",
    "    return imgs_name, true_boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_function(img_obj, true_boxes):\n",
    "    x_img_string = tf.io.read_file(img_obj)\n",
    "    x_img = tf.image.decode_png(x_img_string, channels=3) # dtype=tf.uint8\n",
    "    x_img = tf.image.convert_image_dtype(x_img, tf.float32) # pixel value /255, dtype=tf.float32, channels : RGB\n",
    "    return x_img, true_boxes\n",
    "\n",
    "def get_dataset(imgs_name, bbox, batch_size):\n",
    "    '''\n",
    "    Create a YOLO dataset\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    - ann_dir : annotations files directory\n",
    "    - img_dir : images files directory\n",
    "    - labels : labels list\n",
    "    - batch_size : int\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    - YOLO dataset : generate batch\n",
    "        batch : tupple(images, annotations)\n",
    "        batch[0] : images : tensor (shape : batch_size, IMAGE_W, IMAGE_H, 3)\n",
    "        batch[1] : annotations : tensor (shape : batch_size, max annot, 5)\n",
    "    Note : image pixel values = pixels value / 255. channels : RGB\n",
    "    '''\n",
    "    \n",
    "    # imgs_name, bbox = parse_annotation(ann_dir, img_dir, LABELS)\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((imgs_name, bbox))    \n",
    "    dataset = dataset.shuffle(len(imgs_name))\n",
    "    dataset = dataset.repeat()\n",
    "    dataset = dataset.map(parse_function, num_parallel_calls=6)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(10)\n",
    "    print('-------------------')\n",
    "    print('Dataset:')\n",
    "    print('Images count: {}'.format(len(imgs_name)))\n",
    "    print('Step per epoch: {}'.format(len(imgs_name) // batch_size))\n",
    "    print('Images per epoch: {}'.format(batch_size * (len(imgs_name) // batch_size)))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('imgs_name.pickle', 'rb') as handle:\n",
    "    imgs_name = pickle.load(handle)\n",
    "with open('true_boxes.pickle', 'rb') as handle:\n",
    "    true_boxes = pickle.load(handle)\n",
    "with open('dict_bbxs.pickle', 'rb') as handle:\n",
    "    dict_bbxs = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(imgs_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(true_boxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Risky operation to define all blood cells as cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# category cells\n",
    "\n",
    "cells = []\n",
    "for img_set in true_boxes:\n",
    "    for label in img_set:\n",
    "        label[4] = 0\n",
    "        cells.append(label)\n",
    "            \n",
    "print('cells: ', len(cells))       \n",
    "print('true_boxes: ', len(true_boxes))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox = true_boxes\n",
    "imgs_name_train, imgs_name_test, bbox_train, bbox_test = train_test_split(imgs_name, bbox, test_size = 0.2)\n",
    "train_dataset = None\n",
    "train_dataset= get_dataset(imgs_name_train, bbox_train, TRAIN_BATCH_SIZE)\n",
    "\n",
    "val_dataset = None\n",
    "val_dataset= get_dataset(imgs_name_test, bbox_test, VAL_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test dataset\n",
    "\n",
    "def test_dataset(dataset):\n",
    "    for batch in dataset:\n",
    "        img = batch[0][0]\n",
    "        label = batch[1][0]\n",
    "        plt.figure(figsize=(2,2))\n",
    "        f, (ax1) = plt.subplots(1,1, figsize=(10, 10))\n",
    "        ax1.imshow(img)\n",
    "        ax1.set_title('Input image. Shape : {}'.format(img.shape))\n",
    "        for i in range(label.shape[0]):\n",
    "            box = label[i,:]\n",
    "            box = box.numpy()\n",
    "            x = box[0]\n",
    "            y = box[1]\n",
    "            w = box[2] - box[0]\n",
    "            h = box[3] - box[1]\n",
    "            if box[4] == 1:\n",
    "                color = (1, 0, 0)\n",
    "            elif box[4] == 2:\n",
    "                color = (0, 0, 1)\n",
    "            else:\n",
    "                color = (0, 1, 0)\n",
    "            rect = patches.Rectangle((x, y), w, h, linewidth = 2, edgecolor=color,facecolor='none')\n",
    "            ax1.add_patch(rect)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oversampling \n",
    "https://www.tensorflow.org/tutorials/structured_data/imbalanced_data#oversampling\n",
    "\n",
    "It seems, that we are loosing quite a few labels through the downscaling/ cutting of the images into smaller tiles as we exclude labels that are not 100% visible in the cut image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((imgs_name, bbox))  \n",
    "for features, label in dataset.take(1):\n",
    "    print(label.numpy().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for features, label in dataset.take(1):\n",
    "    print(\"Features:\\n\", features.numpy())\n",
    "    print()\n",
    "    print(\"Label: \", label.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uninfected = []\n",
    "parasitized = []\n",
    "wbc = []\n",
    "for features, labels in dataset:\n",
    "    for label in labels.numpy():\n",
    "        if label[4] == 0 and label[:4].all() != 0:\n",
    "            uninfected.append(label)\n",
    "        if label[4] == 1:\n",
    "            parasitized.append(label)        \n",
    "        if label[4] == 2:\n",
    "            wbc.append(label)\n",
    "            \n",
    "print('uninfected: ', len(uninfected))\n",
    "print('parasitized: ', len(parasitized))\n",
    "print('wbc: ', len(wbc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augmentation_generator(yolo_dataset):\n",
    "    '''\n",
    "    Augmented batch generator from a yolo dataset\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    - YOLO dataset\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    - augmented batch : tensor (shape : batch_size, IMAGE_W, IMAGE_H, 3)\n",
    "        batch : tupple(images, annotations)\n",
    "        batch[0] : images : tensor (shape : batch_size, IMAGE_W, IMAGE_H, 3)\n",
    "        batch[1] : annotations : tensor (shape : batch_size, max annot, 5)\n",
    "    '''\n",
    "    for batch in yolo_dataset:\n",
    "        # conversion tensor->numpy\n",
    "        img = batch[0].numpy()\n",
    "        boxes = batch[1].numpy()\n",
    "        # conversion bbox numpy->ia object\n",
    "        ia_boxes = []\n",
    "        for i in range(img.shape[0]):\n",
    "            ia_bbs = [ia.BoundingBox(x1=bb[0],\n",
    "                                       y1=bb[1],\n",
    "                                       x2=bb[2],\n",
    "                                       y2=bb[3]) for bb in boxes[i]\n",
    "                      if (bb[0] + bb[1] +bb[2] + bb[3] > 0)]\n",
    "            ia_boxes.append(ia.BoundingBoxesOnImage(ia_bbs, shape=(IMAGE_W, IMAGE_H)))\n",
    "        # data augmentation\n",
    "        seq = iaa.Sequential([\n",
    "            iaa.Fliplr(0.5),\n",
    "            iaa.Flipud(0.5),\n",
    "            #iaa.Multiply((0.4, 1.6)), # change brightness\n",
    "            #iaa.ContrastNormalization((0.5, 1.5)),\n",
    "            #iaa.Affine(translate_px={\"x\": (-100,100), \"y\": (-100,100)}, scale=(0.7, 1.30))\n",
    "            ])\n",
    "        #seq = iaa.Sequential([])\n",
    "        seq_det = seq.to_deterministic()\n",
    "        img_aug = seq_det.augment_images(img)\n",
    "        img_aug = np.clip(img_aug, 0, 1)\n",
    "        boxes_aug = seq_det.augment_bounding_boxes(ia_boxes)\n",
    "        # conversion ia object -> bbox numpy\n",
    "        for i in range(img.shape[0]):\n",
    "            boxes_aug[i] = boxes_aug[i].remove_out_of_image().clip_out_of_image()\n",
    "            for j, bb in enumerate(boxes_aug[i].bounding_boxes):\n",
    "                boxes[i,j,0] = bb.x1\n",
    "                boxes[i,j,1] = bb.y1\n",
    "                boxes[i,j,2] = bb.x2\n",
    "                boxes[i,j,3] = bb.y2\n",
    "        # conversion numpy->tensor\n",
    "        batch = (tf.convert_to_tensor(img_aug), tf.convert_to_tensor(boxes))\n",
    "        #batch = (img_aug, boxes)\n",
    "        yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_train_dataset = augmentation_generator(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset(aug_train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Process data to YOLO prediction format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_true_boxes(true_boxes, anchors, image_width, image_height):\n",
    "    '''\n",
    "    Build image ground truth in YOLO format from image true_boxes and anchors.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    - true_boxes : tensor, shape (max_annot, 5), format : x1 y1 x2 y2 c, coords unit : image pixel\n",
    "    - anchors : list [anchor_1_width, anchor_1_height, anchor_2_width, anchor_2_height...]\n",
    "        anchors coords unit : grid cell\n",
    "    - image_width, image_height : int (pixels)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    - detector_mask : array, shape (GRID_W, GRID_H, anchors_count, 1)\n",
    "        1 if bounding box detected by grid cell, else 0\n",
    "    - matching_true_boxes : array, shape (GRID_W, GRID_H, anchors_count, 5)\n",
    "        Contains adjusted coords of bounding box in YOLO format\n",
    "    -true_boxes_grid : array, same shape than true_boxes (max_annot, 5),\n",
    "        format : x, y, w, h, c, coords unit : grid cell\n",
    "        \n",
    "    Note:\n",
    "    -----\n",
    "    Bounding box in YOLO Format : x, y, w, h, c\n",
    "    x, y : center of bounding box, unit : grid cell\n",
    "    w, h : width and height of bounding box, unit : grid cell\n",
    "    c : label index\n",
    "    ''' \n",
    "    \n",
    "    scale = IMAGE_W / GRID_W # scale = 32\n",
    "    \n",
    "    anchors_count = len(anchors) // 2\n",
    "    anchors = np.array(anchors)\n",
    "    anchors = anchors.reshape(len(anchors) // 2, 2)\n",
    "    \n",
    "    detector_mask = np.zeros((GRID_W, GRID_H, anchors_count, 1))\n",
    "    matching_true_boxes = np.zeros((GRID_W, GRID_H, anchors_count, 5))\n",
    "    \n",
    "    # convert true_boxes numpy array -> tensor\n",
    "    true_boxes = true_boxes.numpy()\n",
    "    \n",
    "    true_boxes_grid = np.zeros(true_boxes.shape)\n",
    "    \n",
    "    # convert bounding box coords and localize bounding box\n",
    "    for i, box in enumerate(true_boxes):\n",
    "        # convert box coords to x, y, w, h and convert to grids coord\n",
    "        w = (box[2] - box[0]) / scale\n",
    "        h = (box[3] - box[1]) / scale    \n",
    "        x = ((box[0] + box[2]) / 2) / scale\n",
    "        y = ((box[1] + box[3]) / 2) / scale\n",
    "        true_boxes_grid[i,...] = np.array([x, y, w, h, box[4]])\n",
    "        if w * h > 0: # box exists\n",
    "            # calculate iou between box and each anchors and find best anchors\n",
    "            best_iou = 0\n",
    "            best_anchor = 0\n",
    "            for i in range(anchors_count): \n",
    "                # iou (anchor and box are shifted to 0,0)\n",
    "                intersect = np.minimum(w, anchors[i,0]) * np.minimum(h, anchors[i,1])\n",
    "                union = (anchors[i,0] * anchors[i,1]) + (w * h) - intersect\n",
    "                iou = intersect / union\n",
    "                if iou > best_iou:\n",
    "                    best_iou = iou\n",
    "                    best_anchor = i\n",
    "            # localize box in detector_mask and matching true_boxes\n",
    "            if best_iou > 0:\n",
    "                x_coord = np.floor(x).astype('int')\n",
    "                y_coord = np.floor(y).astype('int')\n",
    "                detector_mask[y_coord, x_coord, best_anchor] = 1\n",
    "                yolo_box = np.array([x, y, w, h, box[4]])\n",
    "                matching_true_boxes[y_coord, x_coord, best_anchor] = yolo_box\n",
    "    return matching_true_boxes, detector_mask, true_boxes_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ground_truth_generator(dataset):\n",
    "    '''\n",
    "    Ground truth batch generator from a yolo dataset, ready to compare with YOLO prediction in loss function.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    - YOLO dataset. Generate batch:\n",
    "        batch : tupple(images, annotations)\n",
    "        batch[0] : images : tensor (shape : batch_size, IMAGE_W, IMAGE_H, 3)\n",
    "        batch[1] : annotations : tensor (shape : batch_size, max annot, 5)\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    - imgs : images to predict. tensor (shape : batch_size, IMAGE_H, IMAGE_W, 3)\n",
    "    - detector_mask : tensor, shape (batch, size, GRID_W, GRID_H, anchors_count, 1)\n",
    "        1 if bounding box detected by grid cell, else 0\n",
    "    - matching_true_boxes : tensor, shape (batch_size, GRID_W, GRID_H, anchors_count, 5)\n",
    "        Contains adjusted coords of bounding box in YOLO format\n",
    "    - class_one_hot : tensor, shape (batch_size, GRID_W, GRID_H, anchors_count, class_count)\n",
    "        One hot representation of bounding box label\n",
    "    - true_boxes_grid : annotations : tensor (shape : batch_size, max annot, 5)\n",
    "        true_boxes format : x, y, w, h, c, coords unit : grid cell\n",
    "    '''\n",
    "    for batch in dataset:\n",
    "        # imgs\n",
    "        imgs = batch[0]\n",
    "        \n",
    "        # true boxes\n",
    "        true_boxes = batch[1]\n",
    "        \n",
    "        # matching_true_boxes and detector_mask\n",
    "        batch_matching_true_boxes = []\n",
    "        batch_detector_mask = []\n",
    "        batch_true_boxes_grid = []\n",
    "        \n",
    "        for i in range(true_boxes.shape[0]):     \n",
    "            one_matching_true_boxes, one_detector_mask, true_boxes_grid = process_true_boxes(true_boxes[i],\n",
    "                                                                                           ANCHORS,\n",
    "                                                                                           IMAGE_W,\n",
    "                                                                                           IMAGE_H)\n",
    "            batch_matching_true_boxes.append(one_matching_true_boxes)\n",
    "            batch_detector_mask.append(one_detector_mask)\n",
    "            batch_true_boxes_grid.append(true_boxes_grid)\n",
    "                \n",
    "        detector_mask = tf.convert_to_tensor(np.array(batch_detector_mask), dtype='float32')\n",
    "        matching_true_boxes = tf.convert_to_tensor(np.array(batch_matching_true_boxes), dtype='float32')\n",
    "        true_boxes_grid = tf.convert_to_tensor(np.array(batch_true_boxes_grid), dtype='float32')\n",
    "        \n",
    "        # class one_hot\n",
    "        matching_classes = K.cast(matching_true_boxes[..., 4], 'int32') \n",
    "        class_one_hot = K.one_hot(matching_classes, CLASS + 1)[:,:,:,:,1:]\n",
    "        class_one_hot = tf.cast(class_one_hot, dtype='float32')\n",
    "        \n",
    "        batch = (imgs, detector_mask, matching_true_boxes, class_one_hot, true_boxes_grid)\n",
    "        yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ground true generator\n",
    "\n",
    "train_gen = ground_truth_generator(aug_train_dataset)\n",
    "val_gen = ground_truth_generator(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Test generator pipeline\n",
    "\n",
    "#model.load_weights('weights/training_do70_2_0.21443991.h5') # best weights, comment to start with YOLO weights\n",
    "\n",
    "# batch\n",
    "img, detector_mask, matching_true_boxes, class_one_hot, true_boxes = next(train_gen)\n",
    "\n",
    "# y\n",
    "matching_true_boxes = matching_true_boxes[0,...]\n",
    "detector_mask = detector_mask[0,...]\n",
    "class_one_hot = class_one_hot[0,...]\n",
    "y = K.concatenate((matching_true_boxes[...,0:4], detector_mask, class_one_hot), axis = -1)\n",
    "\n",
    "# y_hat\n",
    "y_hat = model.predict_on_batch(img)[0,...]\n",
    "\n",
    "# img\n",
    "img = img[0,...]\n",
    "\n",
    "# display prediction (Yolo Confidence value)\n",
    "plt.figure(figsize=(2,2))\n",
    "f, (ax1, ax2, ax3) = plt.subplots(1,3, figsize=(10, 10))\n",
    "ax1.imshow(img)\n",
    "ax1.set_title('Image')\n",
    "\n",
    "ax2.matshow((K.sum(y[:,:,:,4], axis=2))) # YOLO Confidence value\n",
    "ax2.set_title('Ground truth')\n",
    "ax2.xaxis.set_ticks_position('bottom')\n",
    "\n",
    "ax3.matshow(K.sum(y_hat[:,:,:,4], axis=2)) # YOLO Confidence value\n",
    "ax3.set_title('Prediction')\n",
    "ax3.xaxis.set_ticks_position('bottom')\n",
    "\n",
    "f.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Train\n",
    "\n",
    "## 4.1. Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iou(x1, y1, w1, h1, x2, y2, w2, h2):\n",
    "    '''\n",
    "    Calculate IOU between box1 and box2\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    - x, y : box center coords\n",
    "    - w : box width\n",
    "    - h : box height\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    - IOU\n",
    "    '''   \n",
    "    xmin1 = x1 - 0.5*w1\n",
    "    xmax1 = x1 + 0.5*w1\n",
    "    ymin1 = y1 - 0.5*h1\n",
    "    ymax1 = y1 + 0.5*h1\n",
    "    xmin2 = x2 - 0.5*w2\n",
    "    xmax2 = x2 + 0.5*w2\n",
    "    ymin2 = y2 - 0.5*h2\n",
    "    ymax2 = y2 + 0.5*h2\n",
    "    interx = np.minimum(xmax1, xmax2) - np.maximum(xmin1, xmin2)\n",
    "    intery = np.minimum(ymax1, ymax2) - np.maximum(ymin1, ymin2)\n",
    "    inter = interx * intery\n",
    "    union = w1*h1 + w2*h2 - inter\n",
    "    iou = inter / (union + 1e-6)\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss\n",
    "\n",
    "def yolov2_loss(detector_mask, matching_true_boxes, class_one_hot, true_boxes_grid, y_pred, info=False):\n",
    "    '''\n",
    "    Calculate YOLO V2 loss from prediction (y_pred) and ground truth tensors (detector_mask,\n",
    "    matching_true_boxes, class_one_hot, true_boxes_grid,)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    - detector_mask : tensor, shape (batch, size, GRID_W, GRID_H, anchors_count, 1)\n",
    "        1 if bounding box detected by grid cell, else 0\n",
    "    - matching_true_boxes : tensor, shape (batch_size, GRID_W, GRID_H, anchors_count, 5)\n",
    "        Contains adjusted coords of bounding box in YOLO format\n",
    "    - class_one_hot : tensor, shape (batch_size, GRID_W, GRID_H, anchors_count, class_count)\n",
    "        One hot representation of bounding box label\n",
    "    - true_boxes_grid : annotations : tensor (shape : batch_size, max annot, 5)\n",
    "        true_boxes_grid format : x, y, w, h, c (coords unit : grid cell)\n",
    "    - y_pred : prediction from model. tensor (shape : batch_size, GRID_W, GRID_H, anchors count, (5 + labels count)\n",
    "    - info : boolean. True to get some infox about loss value\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    - loss : scalar\n",
    "    - sub_loss : sub loss list : coords loss, class loss and conf loss : scalar\n",
    "    '''\n",
    "    \n",
    "    # anchors tensor\n",
    "    anchors = np.array(ANCHORS)\n",
    "    anchors = anchors.reshape(len(anchors) // 2, 2)\n",
    "    \n",
    "    # grid coords tensor\n",
    "    coord_x = tf.cast(tf.reshape(tf.tile(tf.range(GRID_W), [GRID_H]), (1, GRID_H, GRID_W, 1, 1)), tf.float32)\n",
    "    coord_y = tf.transpose(coord_x, (0,2,1,3,4))\n",
    "    coords = tf.tile(tf.concat([coord_x,coord_y], -1), [y_pred.shape[0], 1, 1, 5, 1])\n",
    "    \n",
    "    # coordinate loss\n",
    "    pred_xy = K.sigmoid(y_pred[:,:,:,:,0:2]) # adjust coords between 0 and 1\n",
    "    pred_xy = (pred_xy + coords) # add cell coord for comparaison with ground truth. New coords in grid cell unit\n",
    "    pred_wh = K.exp(y_pred[:,:,:,:,2:4]) * anchors # adjust width and height for comparaison with ground truth. New coords in grid cell unit\n",
    "    #pred_wh = (pred_wh * anchors) # unit : grid cell\n",
    "    nb_detector_mask = K.sum(tf.cast(detector_mask > 0.0, tf.float32))\n",
    "    xy_loss = LAMBDA_COORD * K.sum(detector_mask * K.square(matching_true_boxes[...,:2] - pred_xy)) / (nb_detector_mask + 1e-6) # Non /2\n",
    "    wh_loss = LAMBDA_COORD * K.sum(detector_mask * K.square(K.sqrt(matching_true_boxes[...,2:4]) - \n",
    "                                                            K.sqrt(pred_wh))) / (nb_detector_mask + 1e-6)\n",
    "    coord_loss = xy_loss + wh_loss\n",
    "    \n",
    "    # class loss    \n",
    "    pred_box_class = y_pred[..., 5:]\n",
    "    true_box_class = tf.argmax(class_one_hot, -1)\n",
    "    #class_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=true_box_class, logits=pred_box_class)\n",
    "    class_loss = K.sparse_categorical_crossentropy(target=true_box_class, output=pred_box_class, from_logits=True)\n",
    "    class_loss = K.expand_dims(class_loss, -1) * detector_mask\n",
    "    class_loss = LAMBDA_CLASS * K.sum(class_loss) / (nb_detector_mask + 1e-6)\n",
    "    \n",
    "    # confidence loss\n",
    "    pred_conf = K.sigmoid(y_pred[...,4:5])\n",
    "    # for each detector : iou between prediction and ground truth\n",
    "    x1 = matching_true_boxes[...,0]\n",
    "    y1 = matching_true_boxes[...,1]\n",
    "    w1 = matching_true_boxes[...,2]\n",
    "    h1 = matching_true_boxes[...,3]\n",
    "    x2 = pred_xy[...,0]\n",
    "    y2 = pred_xy[...,1]\n",
    "    w2 = pred_wh[...,0]\n",
    "    h2 = pred_wh[...,1]\n",
    "    ious = iou(x1, y1, w1, h1, x2, y2, w2, h2)\n",
    "    ious = K.expand_dims(ious, -1)\n",
    "     \n",
    "    # for each detector : best ious between prediction and true_boxes (every bounding box of image)\n",
    "    pred_xy = K.expand_dims(pred_xy, 4) # shape : m, GRID_W, GRID_H, BOX, 1, 2 \n",
    "    pred_wh = K.expand_dims(pred_wh, 4)\n",
    "    pred_wh_half = pred_wh / 2.\n",
    "    pred_mins = pred_xy - pred_wh_half\n",
    "    pred_maxes = pred_xy + pred_wh_half\n",
    "    true_boxe_shape = K.int_shape(true_boxes_grid)\n",
    "    true_boxes_grid = K.reshape(true_boxes_grid, [true_boxe_shape[0], 1, 1, 1, true_boxe_shape[1], true_boxe_shape[2]])\n",
    "    true_xy = true_boxes_grid[...,0:2]\n",
    "    true_wh = true_boxes_grid[...,2:4]\n",
    "    true_wh_half = true_wh * 0.5\n",
    "    true_mins = true_xy - true_wh_half\n",
    "    true_maxes = true_xy + true_wh_half\n",
    "    intersect_mins = K.maximum(pred_mins, true_mins) # shape : m, GRID_W, GRID_H, BOX, max_annot, 2 \n",
    "    intersect_maxes = K.minimum(pred_maxes, true_maxes) # shape : m, GRID_W, GRID_H, BOX, max_annot, 2\n",
    "    intersect_wh = K.maximum(intersect_maxes - intersect_mins, 0.) # shape : m, GRID_W, GRID_H, BOX, max_annot, 1\n",
    "    intersect_areas = intersect_wh[..., 0] * intersect_wh[..., 1] # shape : m, GRID_W, GRID_H, BOX, max_annot, 1\n",
    "    pred_areas = pred_wh[..., 0] * pred_wh[..., 1] # shape : m, GRID_W, GRID_H, BOX, 1, 1\n",
    "    true_areas = true_wh[..., 0] * true_wh[..., 1] # shape : m, GRID_W, GRID_H, BOX, max_annot, 1\n",
    "    union_areas = pred_areas + true_areas - intersect_areas\n",
    "    iou_scores = intersect_areas / union_areas # shape : m, GRID_W, GRID_H, BOX, max_annot, 1\n",
    "    best_ious = K.max(iou_scores, axis=4)  # Best IOU scores.\n",
    "    best_ious = K.expand_dims(best_ious) # shape : m, GRID_W, GRID_H, BOX, 1\n",
    "    \n",
    "    # no object confidence loss\n",
    "    no_object_detection = K.cast(best_ious < 0.6, K.dtype(best_ious)) \n",
    "    noobj_mask = no_object_detection * (1 - detector_mask)\n",
    "    nb_noobj_mask  = K.sum(tf.cast(noobj_mask  > 0.0, tf.float32))\n",
    "    \n",
    "    noobject_loss =  LAMBDA_NOOBJECT * K.sum(noobj_mask * K.square(-pred_conf)) / (nb_noobj_mask + 1e-6)\n",
    "    # object confidence loss\n",
    "    object_loss = LAMBDA_OBJECT * K.sum(detector_mask * K.square(ious - pred_conf)) / (nb_detector_mask + 1e-6)\n",
    "    # total confidence loss\n",
    "    conf_loss = noobject_loss + object_loss\n",
    "    \n",
    "    # total loss\n",
    "    loss = conf_loss + class_loss + coord_loss\n",
    "    sub_loss = [conf_loss, class_loss, coord_loss]  \n",
    "    \n",
    "#     # 'triple' mask\n",
    "#     true_box_conf_IOU = ious * detector_mask\n",
    "#     conf_mask = noobj_mask * LAMBDA_NOOBJECT\n",
    "#     conf_mask = conf_mask + detector_mask * LAMBDA_OBJECT\n",
    "#     nb_conf_box  = K.sum(tf.to_float(conf_mask  > 0.0))\n",
    "#     conf_loss = K.sum(K.square(true_box_conf_IOU - pred_conf) * conf_mask)  / (nb_conf_box  + 1e-6) \n",
    "    \n",
    "#     # total loss\n",
    "#     loss = conf_loss /2. + class_loss + coord_loss /2.\n",
    "#     sub_loss = [conf_loss /2., class_loss, coord_loss /2.]\n",
    "\n",
    "    if info:\n",
    "        print('conf_loss   : {:.4f}'.format(conf_loss))\n",
    "        print('class_loss  : {:.4f}'.format(class_loss))\n",
    "        print('coord_loss  : {:.4f}'.format(coord_loss))\n",
    "        print('    xy_loss : {:.4f}'.format(xy_loss))\n",
    "        print('    wh_loss : {:.4f}'.format(wh_loss))\n",
    "        print('--------------------')\n",
    "        print('total loss  : {:.4f}'.format(loss))\n",
    "        \n",
    "        # display masks for each anchors\n",
    "        for i in range(len(anchors)):\n",
    "            f, (ax1, ax2, ax3) = plt.subplots(1,3, figsize=(10, 5))\n",
    "            f.tight_layout()\n",
    "            f.suptitle('MASKS FOR ANCHOR {} :'.format(anchors[i,...]))\n",
    "            \n",
    "            ax1.matshow((K.sum(detector_mask[0,:,:,i], axis=2)), cmap='Greys', vmin=0, vmax=1)\n",
    "            ax1.set_title('detector_mask, count : {}'.format(K.sum(tf.cast(detector_mask[0,:,:,i]  > 0., tf.int32))))\n",
    "            ax1.xaxis.set_ticks_position('bottom')\n",
    "            \n",
    "            ax2.matshow((K.sum(no_object_detection[0,:,:,i], axis=2)), cmap='Greys', vmin=0, vmax=1)\n",
    "            ax2.set_title('no_object_detection mask')\n",
    "            ax2.xaxis.set_ticks_position('bottom')\n",
    "            \n",
    "            ax3.matshow((K.sum(noobj_mask[0,:,:,i], axis=2)), cmap='Greys', vmin=0, vmax=1)\n",
    "            ax3.set_title('noobj_mask')\n",
    "            ax3.xaxis.set_ticks_position('bottom')\n",
    "              \n",
    "    return loss, sub_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# test loss\n",
    "\n",
    "# get batch\n",
    "img, detector_mask, matching_true_boxes, class_one_hot, true_boxe_grid = next(train_gen)\n",
    "\n",
    "# first image in batch\n",
    "img = img[0:1]\n",
    "detector_mask = detector_mask[0:1]\n",
    "matching_true_boxes = matching_true_boxes[0:1]\n",
    "class_one_hot = class_one_hot[0:1]\n",
    "true_boxe_grid = true_boxe_grid[0:1]\n",
    "\n",
    "# predict\n",
    "y_pred = model.predict_on_batch(img)\n",
    "\n",
    "# plot img, ground truth and prediction\n",
    "f, (ax1, ax2, ax3) = plt.subplots(1,3, figsize=(10, 5))\n",
    "ax1.imshow(img[0,...])\n",
    "ax1.set_title('Image')\n",
    "ax2.matshow(K.sum(detector_mask[0,:,:,:,0], axis=2)) # YOLO Confidence value\n",
    "ax2.set_title('Ground truth, count : {}'.format(K.sum(tf.cast(detector_mask  > 0., tf.int32))))\n",
    "ax2.xaxis.set_ticks_position('bottom')\n",
    "ax3.matshow(K.sum(y_pred[0,:,:,:,4], axis=2)) # YOLO Confidence value\n",
    "ax3.set_title('Prediction')\n",
    "ax3.xaxis.set_ticks_position('bottom')\n",
    "f.tight_layout()\n",
    "\n",
    "# loss info\n",
    "loss, sub_loss = yolov2_loss(detector_mask, matching_true_boxes, class_one_hot, true_boxe_grid, y_pred, info = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradients\n",
    "def grad(model, img, detector_mask, matching_true_boxes, class_one_hot, true_boxes, training=True):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = model(img, training)\n",
    "        loss, sub_loss = yolov2_loss(detector_mask, matching_true_boxes, class_one_hot, true_boxes, y_pred)\n",
    "    return loss, sub_loss, tape.gradient(loss, model.trainable_variables)\n",
    "\n",
    "# save weights\n",
    "def save_best_weights(model, name, val_loss_avg):\n",
    "    # delete existing weights file\n",
    "    files = glob.glob(os.path.join('../data/weights/', name + '*'))\n",
    "#     for file in files:\n",
    "#         os.remove(file)\n",
    "    # create new weights file\n",
    "    name = name + '_' + str(val_loss_avg) + '.h5'\n",
    "    path_name = os.path.join('../data/weights/', name)\n",
    "    model.save_weights(path_name)\n",
    "\n",
    "# log (tensorboard)\n",
    "def log_loss(loss, val_loss, step):\n",
    "    tf.summary.scalar('loss', loss, step)\n",
    "    tf.summary.scalar('val_loss', val_loss, step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "def train(epochs, model, train_dataset, val_dataset, steps_per_epoch_train, steps_per_epoch_val, train_name = 'train'):\n",
    "    '''\n",
    "    Train YOLO model for n epochs.\n",
    "    Eval loss on training and validation dataset.\n",
    "    Log training loss and validation loss for tensorboard.\n",
    "    Save best weights during training (according to validation loss).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    - epochs : integer, number of epochs to train the model.\n",
    "    - model : YOLO model.\n",
    "    - train_dataset : YOLO ground truth and image generator from training dataset.\n",
    "    - val_dataset : YOLO ground truth and image generator from validation dataset.\n",
    "    - steps_per_epoch_train : integer, number of batch to complete one epoch for train_dataset.\n",
    "    - steps_per_epoch_val : integer, number of batch to complete one epoch for val_dataset.\n",
    "    - train_name : string, training name used to log loss and save weights.\n",
    "    \n",
    "    Notes :\n",
    "    - train_dataset and val_dataset generate YOLO ground truth tensors : detector_mask,\n",
    "      matching_true_boxes, class_one_hot, true_boxes_grid. Shape of these tensors (batch size, tensor shape).\n",
    "    - steps per epoch = number of images in dataset // batch size of dataset\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    - loss history : [train_loss_history, val_loss_history] : list of average loss for each epoch.\n",
    "    '''\n",
    "    num_epochs = epochs\n",
    "    steps_per_epoch_train = steps_per_epoch_train\n",
    "    steps_per_epoch_val = steps_per_epoch_val\n",
    "    train_loss_history = []\n",
    "    val_loss_history = []\n",
    "    best_val_loss = 1e6\n",
    "    \n",
    "    # optimizer\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "    \n",
    "    # log (tensorboard)\n",
    "    summary_writer = tf.summary.create_file_writer(os.path.join('../logs/', train_name), flush_millis=20000)\n",
    "    summary_writer.set_as_default()\n",
    "    \n",
    "    # training\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = []\n",
    "        epoch_val_loss = []\n",
    "        epoch_val_sub_loss = []\n",
    "        print('Epoch {} :'.format(epoch))\n",
    "        # train\n",
    "        for batch_idx in range(steps_per_epoch_train): \n",
    "            img, detector_mask, matching_true_boxes, class_one_hot, true_boxes =  next(train_dataset)\n",
    "            loss, _, grads = grad(model, img, detector_mask, matching_true_boxes, class_one_hot, true_boxes)\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "            epoch_loss.append(loss)\n",
    "            print('-', end='')\n",
    "        print(' | ', end='')\n",
    "        # val\n",
    "        for batch_idx in range(steps_per_epoch_val): \n",
    "            img, detector_mask, matching_true_boxes, class_one_hot, true_boxes =  next(val_dataset)\n",
    "            loss, sub_loss, grads = grad(model, img, detector_mask, matching_true_boxes, class_one_hot, true_boxes, training=False)\n",
    "            epoch_val_loss.append(loss)\n",
    "            epoch_val_sub_loss.append(sub_loss)\n",
    "            print('-', end='')\n",
    "\n",
    "        loss_avg = np.mean(np.array(epoch_loss))\n",
    "        val_loss_avg = np.mean(np.array(epoch_val_loss))\n",
    "        sub_loss_avg = np.mean(np.array(epoch_val_sub_loss), axis=0)\n",
    "        train_loss_history.append(loss_avg)\n",
    "        val_loss_history.append(val_loss_avg)\n",
    "        \n",
    "        # log\n",
    "        log_loss(loss_avg, val_loss_avg, epoch)\n",
    "        \n",
    "        # save\n",
    "        if val_loss_avg < best_val_loss:\n",
    "            save_best_weights(model, train_name, val_loss_avg)\n",
    "            best_val_loss = val_loss_avg\n",
    "        \n",
    "        print(' loss = {:.4f}, val_loss = {:.4f} (conf={:.4f}, class={:.4f}, coords={:.4f})'.format(\n",
    "            loss_avg, val_loss_avg, sub_loss_avg[0], sub_loss_avg[1], sub_loss_avg[2]))\n",
    "        \n",
    "    return [train_loss_history, val_loss_history]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "results = train(EPOCHS, model, train_gen, val_gen, 10, 2, 'yolo2')\n",
    "\n",
    "plt.plot(results[0])\n",
    "plt.plot(results[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def display_yolo(file, model, score_threshold, iou_threshold):\n",
    "    '''\n",
    "    Display predictions from YOLO model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    - file : string list : list of images path.\n",
    "    - model : YOLO model.\n",
    "    - score_threshold : threshold used for filtering predicted bounding boxes.\n",
    "    - iou_threshold : threshold used for non max suppression.\n",
    "    '''\n",
    "    # load image\n",
    "    image = cv2.imread(file)\n",
    "\n",
    "    input_image = image[:,:,::-1]\n",
    "    input_image = image / 255.\n",
    "    input_image = np.expand_dims(input_image, 0)\n",
    "\n",
    "    # prediction\n",
    "    y_pred = model.predict_on_batch(input_image)\n",
    "\n",
    "    # post prediction process\n",
    "    # grid coords tensor\n",
    "    coord_x = tf.cast(tf.reshape(tf.tile(tf.range(GRID_W), [GRID_H]), (1, GRID_H, GRID_W, 1, 1)), tf.float32)\n",
    "    coord_y = tf.transpose(coord_x, (0,2,1,3,4))\n",
    "    coords = tf.tile(tf.concat([coord_x,coord_y], -1), [TRAIN_BATCH_SIZE, 1, 1, 5, 1])\n",
    "    dims = K.cast_to_floatx(K.int_shape(y_pred)[1:3])\n",
    "    dims = K.reshape(dims,(1,1,1,1,2))\n",
    "    # anchors tensor\n",
    "    anchors = np.array(ANCHORS)\n",
    "    anchors = anchors.reshape(len(anchors) // 2, 2)\n",
    "    # pred_xy and pred_wh shape (m, GRID_W, GRID_H, Anchors, 2)\n",
    "    pred_xy = K.sigmoid(y_pred[:,:,:,:,0:2])\n",
    "    pred_xy = (pred_xy + coords)\n",
    "    pred_xy = pred_xy / dims\n",
    "    pred_wh = K.exp(y_pred[:,:,:,:,2:4])\n",
    "    pred_wh = (pred_wh * anchors)\n",
    "    pred_wh = pred_wh / dims\n",
    "    # pred_confidence\n",
    "    box_conf = K.sigmoid(y_pred[:,:,:,:,4:5])  \n",
    "    # pred_class\n",
    "    box_class_prob = K.softmax(y_pred[:,:,:,:,5:])\n",
    "\n",
    "    # Reshape\n",
    "    pred_xy = pred_xy[0,...]\n",
    "    pred_wh = pred_wh[0,...]\n",
    "    box_conf = box_conf[0,...]\n",
    "    box_class_prob = box_class_prob[0,...]\n",
    "\n",
    "    # Convert box coords from x,y,w,h to x1,y1,x2,y2\n",
    "    box_xy1 = pred_xy - 0.5 * pred_wh\n",
    "    box_xy2 = pred_xy + 0.5 * pred_wh\n",
    "    boxes = K.concatenate((box_xy1, box_xy2), axis=-1)\n",
    "\n",
    "    # Filter boxes\n",
    "    box_scores = box_conf * box_class_prob\n",
    "    box_classes = K.argmax(box_scores, axis=-1) # best score index\n",
    "    box_class_scores = K.max(box_scores, axis=-1) # best score\n",
    "    prediction_mask = box_class_scores >= score_threshold\n",
    "    boxes = tf.boolean_mask(boxes, prediction_mask)\n",
    "    scores = tf.boolean_mask(box_class_scores, prediction_mask)\n",
    "    classes = tf.boolean_mask(box_classes, prediction_mask)\n",
    "\n",
    "    # Scale box to image shape\n",
    "    boxes = boxes * IMAGE_H\n",
    "\n",
    "    # Non Max Supression\n",
    "    selected_idx = tf.image.non_max_suppression(boxes, scores, 50, iou_threshold=iou_threshold)\n",
    "    boxes = K.gather(boxes, selected_idx)\n",
    "    scores = K.gather(scores, selected_idx)\n",
    "    classes = K.gather(classes, selected_idx)\n",
    "    \n",
    "    # Draw image\n",
    "    plt.figure(figsize=(2,2))\n",
    "    f, (ax1) = plt.subplots(1,1, figsize=(10, 10))\n",
    "    ax1.imshow(image[:,:,::-1])\n",
    "    count_detected = boxes.shape[0]\n",
    "    ax1.set_title('Detected objects count : {}'.format(count_detected))\n",
    "    for i in range(count_detected):\n",
    "        box = boxes[i,...]\n",
    "        x = box[0]\n",
    "        y = box[1]\n",
    "        w = box[2] - box[0]\n",
    "        h = box[3] - box[1]\n",
    "        classe = classes[i].numpy()\n",
    "        if classe == 0:\n",
    "            color = (0, 1, 0)\n",
    "        else:\n",
    "            color = (1, 0, 0)\n",
    "        rect = patches.Rectangle((x.numpy(), y.numpy()), w.numpy(), h.numpy(), linewidth = 3, edgecolor=color,facecolor='none')\n",
    "        ax1.add_patch(rect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    " for img_name_test in imgs_name_test[0:50]:\n",
    "    x_files =  glob.glob(img_name_test)\n",
    "\n",
    "    score = SCORE_THRESHOLD\n",
    "    iou_threshold = IOU_THRESHOLD\n",
    "\n",
    "    for file in x_files[::3]:\n",
    "        display_yolo(file, model, score, iou_threshold)\n",
    "    \n",
    "    img = cv2.imread(img_name_test)\n",
    "    list_bbxs = dict_bbxs[img_name_test]\n",
    "    img_with_box = bbv.draw_multiple_rectangles(img, [box.tolist()[:4] for box in list_bbxs], bbox_color = (255, 0, 0), thickness=1)\n",
    "    plt.imshow(img_with_box)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
